Return-Path: <linux-pci-owner@vger.kernel.org>
X-Original-To: lists+linux-pci@lfdr.de
Delivered-To: lists+linux-pci@lfdr.de
Received: from out1.vger.email (out1.vger.email [IPv6:2620:137:e000::1:20])
	by mail.lfdr.de (Postfix) with ESMTP id D4EB1775029
	for <lists+linux-pci@lfdr.de>; Wed,  9 Aug 2023 03:10:14 +0200 (CEST)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S229874AbjHIBKN (ORCPT <rfc822;lists+linux-pci@lfdr.de>);
        Tue, 8 Aug 2023 21:10:13 -0400
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:37652 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S229548AbjHIBKM (ORCPT
        <rfc822;linux-pci@vger.kernel.org>); Tue, 8 Aug 2023 21:10:12 -0400
Received: from sender4-op-o15.zoho.com (sender4-op-o15.zoho.com [136.143.188.15])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id 1407519AF
        for <linux-pci@vger.kernel.org>; Tue,  8 Aug 2023 18:10:12 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1691543405; cv=none; 
        d=zohomail.com; s=zohoarc; 
        b=k97CAbUDA2SDAuaFbbOMmTAQYGaI5dN4dUCD92Tdyda/FTEnK9cL1v7lyTyql/lxc1c4I0Tyw30wESGCIQfS+26gdBpEj6n64b9G278svZk79Y5LfCXYuYlmh4oNTjd8Br4nG5P6ehDORa2A0+6gB5i3DslAyQwzFKgyuljmGBY=
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=zohomail.com; s=zohoarc; 
        t=1691543405; h=Content-Type:Cc:Date:From:In-Reply-To:MIME-Version:Message-ID:References:Subject:To; 
        bh=Bfk/CkrqobmmlTyw648vhiRlItHsBsRHVV4Mj93MDXA=; 
        b=gtSG3SPSyq7ioWo7LIIK5qwLBCc1aSIvYfrHZ3CBq7DAz8Y9yA2mdmFR+mLR6EczFTjo+R8DaQ5Q2s/1mzArYinyjxukj7q6Fb7RlOxGct82xwQOZGlzrRMa/euc8MgeMz834v8s2nnwZz/Cph5hkcc+DCF16iHn+hECQozkWe4=
ARC-Authentication-Results: i=1; mx.zohomail.com;
        dkim=pass  header.i=linux.beauty;
        spf=pass  smtp.mailfrom=me@linux.beauty;
        dmarc=pass header.from=<me@linux.beauty>
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed; t=1691543405;
        s=zmail; d=linux.beauty; i=me@linux.beauty;
        h=Date:Date:Message-ID:From:From:To:To:Cc:Cc:Subject:Subject:In-Reply-To:References:MIME-Version:Content-Type:Message-Id:Reply-To;
        bh=Bfk/CkrqobmmlTyw648vhiRlItHsBsRHVV4Mj93MDXA=;
        b=nSdyQJgw96Qu+db4dqc8pEW3AjjysOpTgyL0Mg+R8QDWmMkCLR+dIavKdseAQLde
        gKEHTAJ/q9LsrLjK254uBpPneI0y87soZsBeSAfmdaZZzDCHPNTqV2jmxvtJ/ssBkz5
        qAdSv7R5dDAVh7yxWjABxgeyugG+bCq6kcfER8XU=
Received: from lchen-ArchLinux.linux.beauty (116.246.37.178 [116.246.37.178]) by mx.zohomail.com
        with SMTPS id 1691543403842388.3545741138905; Tue, 8 Aug 2023 18:10:03 -0700 (PDT)
Date:   Wed, 09 Aug 2023 09:09:45 +0800
Message-ID: <87sf8t58di.wl-me@linux.beauty>
From:   Li Chen <me@linux.beauty>
To:     "Arnd Bergmann" <arnd@arndb.de>
Cc:     "Manivannan Sadhasivam" <mani@kernel.org>,
        linux-pci <linux-pci@vger.kernel.org>,
        "Lorenzo Pieralisi" <lpieralisi@kernel.org>,
        "Kishon Vijay Abraham I" <kishon@ti.com>,
        "Bjorn Helgaas" <helgaas@kernel.org>
Subject: Re: [RFC] add __iomem cookie for EPF BAR
In-Reply-To: <344b8ccd-23c9-4476-9493-1d2b9c23590d@app.fastmail.com>
References: <189cff865f3.fc7e71c96186.1411633612292556520@linux.beauty>
        <20230808070357.GC4990@thinkpad>
        <344b8ccd-23c9-4476-9493-1d2b9c23590d@app.fastmail.com>
User-Agent: Wanderlust/2.15.9 (Almost Unreal) SEMI-EPG/1.14.7 (Harue)
 FLIM-LB/1.14.9 (=?ISO-8859-4?Q?Goj=F2?=) APEL-LB/10.8 EasyPG/1.0.0
 Emacs/28.2 (x86_64-pc-linux-gnu) MULE/6.0 (HANACHIRUSATO)
MIME-Version: 1.0 (generated by SEMI-EPG 1.14.7 - "Harue")
Content-Type: text/plain; charset=US-ASCII
X-ZohoMailClient: External
X-Spam-Status: No, score=-2.1 required=5.0 tests=BAYES_00,DKIM_SIGNED,
        DKIM_VALID,DKIM_VALID_AU,DKIM_VALID_EF,RCVD_IN_MSPIKE_H2,SPF_HELO_NONE,
        SPF_PASS,URIBL_BLOCKED autolearn=ham autolearn_force=no version=3.4.6
X-Spam-Checker-Version: SpamAssassin 3.4.6 (2021-04-09) on
        lindbergh.monkeyblade.net
Precedence: bulk
List-ID: <linux-pci.vger.kernel.org>
X-Mailing-List: linux-pci@vger.kernel.org

On Tue, 08 Aug 2023 15:44:44 +0800,
Arnd Bergmann wrote:
Hi Arnd,
> 
> On Tue, Aug 8, 2023, at 09:03, mani wrote:
> > On Mon, Aug 07, 2023 at 08:28:30PM +0800, Li Chen wrote:
> >> 
> >> Currently, the EPF's bar is allocated by pci_epf_alloc_space, which internally uses dma_alloc_coherent and the caching behavior of dma_alloc_coherent may vary depending on platforms.
> >> 
> >> The bar space is exported to RC, which means that RC may modify it without EP being aware of it, so EP still read from the cache and get stalled data. To address this issue, the bar space should be treated as iomem instead and forced to use io read/write APIs, which enforces volatile. 
> >> 
> >
> > We already had a similar discussion on using volatile for BAR space and settled
> > with {WRITE/READ}_ONCE macros in EPF Test driver [1].
> >
> > Since the BAR space allocated in endpoint is not a MMIO, I don't think it should
> > be forced as iomem. Rather EPF drivers should use _ONCE macros to access the
> > fields to avoid coherency issues as suggested by Arnd earlier.
> 
> Using readl/writel is clearly the wrong solution here as I explained
> before, but I assume that Li Chen is dealing with a real problem.

Thanks, I learnt much from your mail.
Actually, I'm not dealing with a real problem.

> If the cache is coherent with the device, then reading from the cache
> is clearly the right thing to do,

I guess that even SoCs with CCI support might not handle cache for RC
access if specific bus interfaces are not connected.

> but the mentioned "stall" problem may
> be related to the store buffers, where an dma_wmb() after the
> WRITE_ONCE() is missing. Similarly, a dma_rmb() might be missing before
> a READ_ONCE() to prevent prefetching during out-of-order execution.
> 
> With readl()/writel(), you already get very heavy barriers, so it may
> end up working by accident, but these barriers are at the other side
> of the access (before writel and after readl) and may be the wrong
> type of barrier depending on the CPU.

For systems that aren't cache-coherent, is it accurate to say that the store
buffer might still be utilized, and that there might still be a need for dma_wmb and dma_rmb?
